{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "general_ko.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import argparse\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "\n",
        "U_TKN = '<usr>'\n",
        "S_TKN = '<sys>'\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'\n",
        "\n",
        "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
        "            pad_token=PAD, mask_token=MASK) \n",
        "\n",
        "\n",
        "class KoGPT2Chat(LightningModule):\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super(KoGPT2Chat, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.neg = -1e18\n",
        "        self.kogpt2 = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (batch, seq_len, hiddens)\n",
        "        output = self.kogpt2(inputs, return_dict=True)\n",
        "        return output.logits\n",
        "\n",
        "    def chat(self, sent='0'):\n",
        "        tok = TOKENIZER\n",
        "        sent_tokens = tok.tokenize(sent)\n",
        "        with torch.no_grad():\n",
        "            choice = random.sample(keywords, 3)\n",
        "            q = str(choice[0] + ', ' + choice[1] + ', ' + choice[2]).strip()\n",
        "            a = ''\n",
        "            while 1:\n",
        "                input_ids = torch.LongTensor(tok.encode(U_TKN + q + SENT + sent + S_TKN + a)).unsqueeze(dim=0)\n",
        "                pred = self(input_ids)\n",
        "                gen = tok.convert_ids_to_tokens(\n",
        "                    torch.argmax(\n",
        "                        pred,\n",
        "                        dim=-1).squeeze().numpy().tolist())[-1]\n",
        "                if gen == EOS:\n",
        "                    break\n",
        "                a += gen.replace('▁', ' ')\n",
        "            print(\"output : {}\".format(a.strip()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = KoGPT2Chat.load_from_checkpoint('model_chp/model_-last.ckpt')\n",
        "    # 위 코드는 모델을 처음에 한번 불러오는 코드로 만들것\n",
        "    \n",
        "    aa = pd.read_csv('Chatbot_data/keyword.csv')\n",
        "    keywords = aa['keyword'].to_list()\n",
        "\n",
        "    model.chat()\n",
        "    # model.chat()은 클라이언트에서 요청이 오면 반환값을 받아서 넘기면 됨(요청 올때만 실행시키면 될듯!)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfqEA3h9oeLL",
        "outputId": "d90939bd-ad47-47d3-cf14-c68b8f910ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output : 겸손함은 고용된 사람이다.\n"
          ]
        }
      ]
    }
  ]
}